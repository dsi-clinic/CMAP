{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.append(\"/home/gregoryc25/CMAP/segment_anything_source_code\")\n",
    "\n",
    "from segment_anything.build_sam import sam_model_registry\n",
    "from segment_anything.predictor import SamPredictor\n",
    "\n",
    "# Define checkpoint path\n",
    "home_dir = Path.home()\n",
    "sam_checkpoint = home_dir / \"CMAP/segment_anything_source_code/sam_vit_h.pth\"\n",
    "\n",
    "# Ensure the checkpoint file exists\n",
    "if not sam_checkpoint.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found: {sam_checkpoint}\")\n",
    "\n",
    "# Load the SAM ViT-H model and create the predictor\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=str(sam_checkpoint))\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sam.to(device)\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 image-mask pairs out of 50 images.\n",
      "Loaded 50 training samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class AerialDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, mask_prefix=\"mask_\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.mask_prefix = mask_prefix\n",
    "        \n",
    "        # List all image files with .tif extension\n",
    "        self.all_image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(\".tif\")])\n",
    "        self.image_files = []\n",
    "        self.mask_files = []\n",
    "        for f in self.all_image_files:\n",
    "            mask_file = self.mask_prefix + f  # prepend 'mask_' to the image filename\n",
    "            mask_path = os.path.join(mask_dir, mask_file)\n",
    "            if os.path.exists(mask_path):\n",
    "                self.image_files.append(f)\n",
    "                self.mask_files.append(mask_file)\n",
    "            else:\n",
    "                print(f\"Warning: mask for {f} not found, expecting {mask_file}\")\n",
    "        print(f\"Found {len(self.image_files)} image-mask pairs out of {len(self.all_image_files)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        \n",
    "        # Open image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask_img = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        # Resize image and mask so that the longest side equals 1024 pixels\n",
    "        w, h = image.size\n",
    "        r = min(1024.0 / w, 1024.0 / h)\n",
    "        new_w, new_h = int(w * r), int(h * r)\n",
    "        if (new_w, new_h) != (w, h):\n",
    "            image = image.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "            mask_img = mask_img.resize((new_w, new_h), resample=Image.NEAREST)\n",
    "        \n",
    "        image_np = np.array(image)\n",
    "        mask_np = np.array(mask_img)\n",
    "        \n",
    "        # Binarize mask (all non-zero as foreground)\n",
    "        mask_np = (mask_np > 0).astype(np.uint8)\n",
    "        \n",
    "        # Erode mask to avoid selecting boundary points (using a 5x5 kernel)\n",
    "        if mask_np.max() > 0:\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            eroded_mask = cv2.erode(mask_np, kernel, iterations=1)\n",
    "        else:\n",
    "            eroded_mask = mask_np\n",
    "        if eroded_mask.max() == 0:\n",
    "            eroded_mask = mask_np\n",
    "        \n",
    "        # Sample a random foreground point from the eroded mask\n",
    "        coords = np.argwhere(eroded_mask > 0)\n",
    "        if coords.size == 0:\n",
    "            point = np.array([0, 0], dtype=np.int32)\n",
    "        else:\n",
    "            iy, ix = coords[np.random.randint(len(coords))]\n",
    "            point = np.array([int(ix), int(iy)], dtype=np.int32)\n",
    "        \n",
    "        return image_np, mask_np, point\n",
    "\n",
    "train_image_dir = \"/net/projects/cmap/data/KC-images\"\n",
    "train_mask_dir  = \"/net/projects/cmap/data/KC-masks/single-band-masks\"\n",
    "\n",
    "# Create dataset with updated naming convention\n",
    "train_dataset = AerialDataset(train_image_dir, train_mask_dir)\n",
    "\n",
    "# collate function\n",
    "def sam_collate_fn(batch):\n",
    "    images, masks, points = zip(*batch)\n",
    "    return list(images), list(masks), list(points)\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, collate_fn=sam_collate_fn)\n",
    "    print(f\"Loaded {len(train_dataset)} training samples.\")\n",
    "else:\n",
    "    print(\"Error: No training samples found. Please check your mask directory or file naming convention.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model encoders frozen. Optimizer for mask decoder is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3868928/2731223266.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Freeze image and prompt encoders; only fine-tune the mask decoder\n",
    "for param in sam.image_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in sam.prompt_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "sam.image_encoder.eval()\n",
    "sam.prompt_encoder.eval()\n",
    "sam.mask_decoder.train()\n",
    "\n",
    "# Set up optimizer for the mask decoder only\n",
    "optimizer = torch.optim.AdamW(sam.mask_decoder.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Initialize mixed-precision gradient scaler (if running on GPU)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"Model encoders frozen. Optimizer for mask decoder is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (124382208 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (123734800 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (126762952 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (125483745 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (127456440 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3868928/1493150246.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (124865752 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (125787662 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (124736000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (127109475 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (123191171 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (122658134 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (125048595 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (125250755 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (124385040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "num_epochs       = 5\n",
    "grad_accum_steps = 4\n",
    "log_interval     = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_iou  = 0.0\n",
    "\n",
    "    for i, (images, masks, points) in enumerate(train_loader, start=1):\n",
    "        # 1) Unpack sample\n",
    "        image_np   = images[0]\n",
    "        gt_mask_np = masks[0]\n",
    "        point      = points[0]\n",
    "\n",
    "        # 2) Cache image embedding\n",
    "        predictor.set_image(image_np)\n",
    "        image_embedding = predictor.get_image_embedding().to(device)\n",
    "\n",
    "        # 3) Prepare prompts\n",
    "        input_point = np.expand_dims(point, 0).astype(np.int32)\n",
    "        input_label = np.array([[1]], dtype=np.int32)\n",
    "        pt_t = torch.from_numpy(input_point).to(device).unsqueeze(0)\n",
    "        lbl_t = torch.from_numpy(input_label).to(device)\n",
    "\n",
    "        # 4) Encode prompts\n",
    "        sparse_emb, dense_emb = sam.prompt_encoder(\n",
    "            (pt_t, lbl_t), None, None\n",
    "        )\n",
    "\n",
    "        # 5) Forward through decoder\n",
    "        with torch.cuda.amp.autocast():\n",
    "            low_res_masks, iou_scores = sam.mask_decoder(\n",
    "                image_embeddings         = image_embedding,\n",
    "                image_pe                 = sam.prompt_encoder.get_dense_pe().to(device),\n",
    "                sparse_prompt_embeddings = sparse_emb.to(device),\n",
    "                dense_prompt_embeddings  = dense_emb.to(device),\n",
    "                multimask_output         = True,\n",
    "            )\n",
    "            low_res = low_res_masks[:, 0:1, ...]\n",
    "            H, W = gt_mask_np.shape\n",
    "            up_mask = torch.nn.functional.interpolate(\n",
    "                low_res, size=(H, W),\n",
    "                mode='bilinear', align_corners=False\n",
    "            )\n",
    "\n",
    "            # **Make GT tensor [1,1,H,W]**\n",
    "            gt_tensor = torch.from_numpy(gt_mask_np).float().to(device)\n",
    "            gt_tensor = gt_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(up_mask, gt_tensor)\n",
    "\n",
    "        # 6) Backprop + optimizer step\n",
    "        scaler.scale(loss).backward()\n",
    "        if i % grad_accum_steps == 0 or i == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 7) Compute IoU\n",
    "        with torch.no_grad():\n",
    "            pred = (torch.sigmoid(up_mask) > 0.5).float()\n",
    "            inter = (pred * gt_tensor).sum().item()\n",
    "            union = (pred + gt_tensor - pred*gt_tensor).sum().item()\n",
    "            running_iou += (inter / union) if union > 0 else 0.0\n",
    "\n",
    "        # 8) Log\n",
    "        if i % log_interval == 0:\n",
    "            avg_l = running_loss / i\n",
    "            avg_i = running_iou  / i\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Step [{i}/{len(train_loader)}] â€“ \"\n",
    "                  f\"Loss: {avg_l:.4f}, IoU: {avg_i:.4f}\")\n",
    "\n",
    "    # end of epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_iou  = running_iou  / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} complete: Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated model's state_dict (which now contains the fine-tuned mask decoder)\n",
    "save_path = \"sam_vit_h_finetuned.pth\"\n",
    "torch.save(sam.state_dict(), save_path)\n",
    "print(f\"Fine-tuned model saved as {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
