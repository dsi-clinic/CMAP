{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.append(\"/home/gregoryc25/CMAP/segment_anything_source_code\")\n",
    "\n",
    "from segment_anything.build_sam import sam_model_registry\n",
    "from segment_anything.predictor import SamPredictor\n",
    "\n",
    "# Define checkpoint path\n",
    "home_dir = Path.home()\n",
    "sam_checkpoint = home_dir / \"CMAP/segment_anything_source_code/sam_vit_h.pth\"\n",
    "\n",
    "# Ensure the checkpoint file exists\n",
    "if not sam_checkpoint.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found: {sam_checkpoint}\")\n",
    "\n",
    "# Load the SAM ViT-H model and create the predictor\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=str(sam_checkpoint))\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sam.to(device)\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 image-mask pairs out of 50 images.\n",
      "Loaded 50 training samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class AerialDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, mask_prefix=\"mask_\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.mask_prefix = mask_prefix\n",
    "        \n",
    "        # List all image files with .tif extension\n",
    "        self.all_image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(\".tif\")])\n",
    "        self.image_files = []\n",
    "        self.mask_files = []\n",
    "        for f in self.all_image_files:\n",
    "            mask_file = self.mask_prefix + f  # prepend 'mask_' to the image filename\n",
    "            mask_path = os.path.join(mask_dir, mask_file)\n",
    "            if os.path.exists(mask_path):\n",
    "                self.image_files.append(f)\n",
    "                self.mask_files.append(mask_file)\n",
    "            else:\n",
    "                print(f\"Warning: mask for {f} not found, expecting {mask_file}\")\n",
    "        print(f\"Found {len(self.image_files)} image-mask pairs out of {len(self.all_image_files)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        \n",
    "        # Open image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask_img = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        # Resize image and mask so that the longest side equals 1024 pixels\n",
    "        w, h = image.size\n",
    "        r = min(1024.0 / w, 1024.0 / h)\n",
    "        new_w, new_h = int(w * r), int(h * r)\n",
    "        if (new_w, new_h) != (w, h):\n",
    "            image = image.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "            mask_img = mask_img.resize((new_w, new_h), resample=Image.NEAREST)\n",
    "        \n",
    "        image_np = np.array(image)\n",
    "        mask_np = np.array(mask_img)\n",
    "        \n",
    "        # Binarize mask (all non-zero as foreground)\n",
    "        mask_np = (mask_np > 0).astype(np.uint8)\n",
    "        \n",
    "        # Erode mask to avoid selecting boundary points (using a 5x5 kernel)\n",
    "        if mask_np.max() > 0:\n",
    "            kernel = np.ones((5, 5), np.uint8)\n",
    "            eroded_mask = cv2.erode(mask_np, kernel, iterations=1)\n",
    "        else:\n",
    "            eroded_mask = mask_np\n",
    "        if eroded_mask.max() == 0:\n",
    "            eroded_mask = mask_np\n",
    "        \n",
    "        # Sample a random foreground point from the eroded mask\n",
    "        coords = np.argwhere(eroded_mask > 0)\n",
    "        if coords.size == 0:\n",
    "            point = np.array([0, 0], dtype=np.int32)\n",
    "        else:\n",
    "            iy, ix = coords[np.random.randint(len(coords))]\n",
    "            point = np.array([int(ix), int(iy)], dtype=np.int32)\n",
    "        \n",
    "        return image_np, mask_np, point\n",
    "\n",
    "train_image_dir = \"/net/projects/cmap/data/KC-images\"\n",
    "train_mask_dir  = \"/net/projects/cmap/data/KC-masks/single-band-masks\"\n",
    "\n",
    "# Create dataset with updated naming convention\n",
    "train_dataset = AerialDataset(train_image_dir, train_mask_dir)\n",
    "\n",
    "# collate function\n",
    "def sam_collate_fn(batch):\n",
    "    images, masks, points = zip(*batch)\n",
    "    return list(images), list(masks), list(points)\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, collate_fn=sam_collate_fn)\n",
    "    print(f\"Loaded {len(train_dataset)} training samples.\")\n",
    "else:\n",
    "    print(\"Error: No training samples found. Please check your mask directory or file naming convention.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model encoders frozen. Optimizer for mask decoder is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1811253/576430641.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Freeze image and prompt encoders; only fine-tune the mask decoder\n",
    "for param in sam.image_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in sam.prompt_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "sam.image_encoder.eval()\n",
    "sam.prompt_encoder.eval()\n",
    "sam.mask_decoder.train()\n",
    "\n",
    "# Set up optimizer for the mask decoder only\n",
    "optimizer = torch.optim.AdamW(sam.mask_decoder.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Initialize mixed-precision gradient scaler (if running on GPU)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"Model encoders frozen. Optimizer for mask decoder is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (126491808 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (124804146 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (126545454 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (124791273 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "/home/gregoryc25/micromamba/envs/cmap/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (127043289 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PromptEncoder' object has no attribute 'forward_points'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m input_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Generate prompt embeddings from the prompt encoder.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Using the forward_points function (adjust if your version has a different API)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_points\u001b[49m(input_point, input_label)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Retrieve the cached image embedding from the predictor.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# (SamPredictor stores image embeddings in predictor.features[\"image_embedding\"])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m image_embedding \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# add batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/cmap/lib/python3.10/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PromptEncoder' object has no attribute 'forward_points'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_epochs = 5          \n",
    "grad_accum_steps = 4 # Update parameters every 4 iterations (simulate larger batch size)\n",
    "log_interval = 10  \n",
    "\n",
    "# Training loop: iterate over the DataLoader\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    for i, (images, masks, points) in enumerate(train_loader):\n",
    "        # For batch size 1, extract the single sample\n",
    "        image_np = images[0]       # numpy array of shape (H, W, 3)\n",
    "        gt_mask_np = masks[0]        # numpy array of shape (H, W), binary mask (0,1)\n",
    "        point = points[0]          # numpy array of shape (2,)\n",
    "        \n",
    "        # Set the image in the predictor to compute and cache the image embeddings.\n",
    "        predictor.set_image(image_np)\n",
    "        \n",
    "        # Prepare point prompt: ensure shape (1,2) and label (1,1) for a foreground prompt.\n",
    "        input_point = np.expand_dims(point, axis=0).astype(np.int32)\n",
    "        input_label = np.array([[1]], dtype=np.int32)\n",
    "        \n",
    "        # Generate prompt embeddings from the prompt encoder.\n",
    "        sparse_embeddings, dense_embeddings = sam.prompt_encoder.forward_points(input_point, input_label)\n",
    "        \n",
    "        # Retrieve the cached image embedding from the predictor.\n",
    "        # (SamPredictor stores image embeddings in predictor.features[\"image_embedding\"])\n",
    "        image_embedding = predictor.features[\"image_embedding\"].unsqueeze(0)  # add batch dimension\n",
    "        \n",
    "        # Forward pass through the mask decoder with mixed precision.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            low_res_masks, iou_scores, _, _ = sam.mask_decoder.forward(\n",
    "                image_embeddings = image_embedding.to(device),\n",
    "                image_pe = sam.prompt_encoder.get_dense_pe().to(device),\n",
    "                sparse_prompt_embeddings = sparse_embeddings.to(device),\n",
    "                dense_prompt_embeddings = dense_embeddings.to(device),\n",
    "                multimask_output = True\n",
    "            )\n",
    "            # Select the first candidate mask (shape: (1, 1, H_low, W_low))\n",
    "            low_res_mask = low_res_masks[:, 0:1, :, :]\n",
    "            \n",
    "            # Upsample the predicted mask to the ground-truth mask size.\n",
    "            H, W = gt_mask_np.shape\n",
    "            up_mask = torch.nn.functional.interpolate(low_res_mask, size=(H, W), mode='bilinear', align_corners=False)\n",
    "            # up_mask contains logits.\n",
    "            \n",
    "            # Convert ground-truth mask to torch tensor.\n",
    "            gt_mask_tensor = torch.from_numpy(gt_mask_np).float().to(device).unsqueeze(0)\n",
    "            \n",
    "            # Compute binary cross-entropy loss (with logits).\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(up_mask, gt_mask_tensor)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update model parameters every grad_accum_steps iterations.\n",
    "        if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Compute Intersection over Union (IoU) for monitoring.\n",
    "        with torch.no_grad():\n",
    "            pred_mask = (torch.sigmoid(up_mask) > 0.5).float()\n",
    "            intersection = (pred_mask * gt_mask_tensor).sum().item()\n",
    "            union = (pred_mask + gt_mask_tensor - pred_mask * gt_mask_tensor).sum().item()\n",
    "            sample_iou = intersection / union if union > 0 else 0.0\n",
    "        running_iou += sample_iou\n",
    "        \n",
    "        if (i + 1) % log_interval == 0:\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "            avg_iou = running_iou / (i + 1)\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}] - Loss: {avg_loss:.4f}, IoU: {avg_iou:.4f}\")\n",
    "    \n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "    avg_epoch_iou = running_iou / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} complete: Avg Loss: {avg_epoch_loss:.4f}, Avg IoU: {avg_epoch_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated model's state_dict (which now contains the fine-tuned mask decoder)\n",
    "save_path = \"sam_vit_h_finetuned.pth\"\n",
    "torch.save(sam.state_dict(), save_path)\n",
    "print(f\"Fine-tuned model saved as {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
